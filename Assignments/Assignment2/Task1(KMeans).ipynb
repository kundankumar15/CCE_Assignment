{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQn7gC9wFlgm"
      },
      "source": [
        "# K-means Clustering\n",
        "\n",
        "In this assignment, you will implement the K-Means clustering algorithm step by step. The goal is to gain a deeper understanding of how K-Means works by coding each part of the algorithm from scratch. You will:\n",
        "\n",
        "1. Find the closest centroids for each data point.\n",
        "2. Compute the new centroid means based on assignments.\n",
        "3. Determine the optimal number of clusters (K) using the Elbow Method.\n",
        "4. Integrate all steps into a single function `run_kmeans()` and visualize the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vIl4h2w2Flgo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMxJxWRRFlgp"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "## 1 - Implementing K-means\n",
        "\n",
        "The K-means algorithm is a method to automatically cluster similar\n",
        "data points together.\n",
        "\n",
        "* Concretely, you are given a training set $\\{x^{(1)}, ..., x^{(m)}\\}$, and you want\n",
        "to group the data into a few cohesive “clusters”.\n",
        "      \n",
        "* In pseudocode, the K-means algorithm is as follows:\n",
        "\n",
        "    ``` python\n",
        "    # Initialize centroids\n",
        "    # K is the number of clusters\n",
        "    centroids = kMeans_init_centroids(X, K)\n",
        "    \n",
        "    for iter in range(iterations):\n",
        "        # Cluster assignment step:\n",
        "        # Assign each data point to the closest centroid.\n",
        "        # idx[i] corresponds to the index of the centroid\n",
        "        # assigned to example i\n",
        "        idx = find_closest_centroids(X, centroids)\n",
        "\n",
        "        # Move centroid step:\n",
        "        # Compute means based on centroid assignments\n",
        "        centroids = compute_means(X, idx, K)\n",
        "    ```\n",
        "* The inner-loop of the algorithm repeatedly carries out two steps:\n",
        "    * (i) Assigning each training example $x^{(i)}$ to its closest centroid, and\n",
        "    * (ii) Recomputing the mean of each centroid using the points assigned to it.\n",
        "\n",
        "* The $K$-means algorithm will always converge to some final set of means for the centroids.\n",
        "\n",
        "* However, that the converged solution may not always be ideal and depends on the initial setting of the centroids.\n",
        "    * Therefore, in practice the K-means algorithm is usually run a few times with different random initializations.\n",
        "    * One way to choose between these different solutions from different random initializations is to choose the one with the lowest cost function value (distortion).\n",
        "\n",
        "* You will implement the two phases of the K-means algorithm separately\n",
        "in the next sections.\n",
        "    * You will start by completing `find_closest_centroid` and then proceed to complete `compute_centroids`.\n",
        "\n",
        "* **Determining the Optimal Number of Clusters (K) using the Elbow Method**:\n",
        "    * After completing the basic K-means algorithm implementation, you will utilize the Elbow Method to determine the optimal number of clusters.\n",
        "    * This involves plotting the cost (sum of squared distances) versus the number of clusters and looking for a 'knee' in the graph. The knee or elbow point is typically considered as the optimal number of clusters where increasing the number of clusters does not significantly decrease the cost function, suggesting diminishing returns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUkbK7TMFlgr"
      },
      "source": [
        "<a name=\"1.1\"></a>\n",
        "### 1.1 Finding closest centroids\n",
        "\n",
        "The algorithm assigns every training example $x^{(i)}$ to its closest\n",
        "centroid, given the current positions of centroids.\n",
        "\n",
        "<a name=\"ex01\"></a>\n",
        "### Exercise 1\n",
        "\n",
        "Your task is to complete the code in `find_closest_centroids`.\n",
        "* This function takes the data matrix `X` and the locations of all\n",
        "centroids inside `centroids`\n",
        "* It should output a one-dimensional array `idx` (which has the same number of elements as `X`) that holds the index  of the closest centroid (a value in $\\{1,...,K\\}$, where $K$ is total number of centroids) to every training example .\n",
        "* Specifically, for every example $x^{(i)}$ we set\n",
        "$$c^{(i)} := j \\quad \\mathrm{that \\; minimizes} \\quad ||x^{(i)} - \\mu_j||^2,$$\n",
        "where\n",
        " * $c^{(i)}$ is the index of the centroid that is closest to $x^{(i)}$ (corresponds to `idx[i]` in the starter code), and\n",
        " * $\\mu_j$ is the position (value) of the $j$’th centroid. (stored in `centroids` in the starter code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WWAGaVcSFlgr"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION 1: find_closest_centroids\n",
        "\n",
        "def find_closest_centroids(X, centroids):\n",
        "    \"\"\"\n",
        "    Assign each data point to the closest centroid\n",
        "\n",
        "    Input:\n",
        "    X: ndarray of shape (m, n) - m data points with n features\n",
        "    centroids: ndarray of shape (K, n) - K centroids with n features\n",
        "\n",
        "    Output:\n",
        "    idx: ndarray of shape (m,) - index of the closest centroid for each data point\n",
        "    \"\"\"\n",
        "    # Set K\n",
        "    K = centroids.shape[0]\n",
        "\n",
        "    # You need to return the following variables correctly\n",
        "    idx = np.zeros(X.shape[0], dtype=int)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    for i in range(X.shape[0]):\n",
        "        # Array to hold distance between X[i] and each centroids[j]\n",
        "        distance = []\n",
        "        for j in range(K):\n",
        "            norm_ij = # Your code to calculate the norm between (X[i] - centroids[j]) ; Squared Euclidean distance\n",
        "            distance.append(norm_ij)\n",
        "\n",
        "        idx[i] = # Your code here to calculate index of minimum value in distance\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv6a0f65Flgs"
      },
      "source": [
        "Now let's check your implementation using an example dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST FUNCTION**"
      ],
      "metadata": {
        "id": "c5_b953Ji3v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simply Run this cell.Do not Touch this code\n",
        "# Test function\n",
        "def find_closest_centroids_test(target):\n",
        "    print(\"Running tests for find_closest_centroids:\")\n",
        "\n",
        "    # Test case 1: Simple 2D example\n",
        "    X = np.array([[1, 1], [2, 1], [4, 3], [5, 4]])\n",
        "    centroids = np.array([[1, 2], [5, 3]])\n",
        "    idx = target(X, centroids)\n",
        "    expected_idx = np.array([0, 0, 1, 1])\n",
        "\n",
        "    print(\"\\nTest case 1:\")\n",
        "    print(\"Input X:\", X)\n",
        "    print(\"Input centroids:\", centroids)\n",
        "    print(\"Expected output:\", expected_idx)\n",
        "    print(\"Actual output:\", idx)\n",
        "\n",
        "    assert np.all(idx == expected_idx), f\"Test case 1 failed. Expected: {expected_idx}, got: {idx}\"\n",
        "\n",
        "    # Test case 2: 3D example\n",
        "    X = np.array([[1, 1, 1], [2, 2, 2], [4, 4, 4], [5, 5, 5]])\n",
        "    centroids = np.array([[1, 1, 1], [4, 4, 4]])\n",
        "    idx = target(X, centroids)\n",
        "    expected_idx = np.array([0, 0, 1, 1])\n",
        "\n",
        "    print(\"\\nTest case 2:\")\n",
        "    print(\"Input X:\", X)\n",
        "    print(\"Input centroids:\", centroids)\n",
        "    print(\"Expected output:\", expected_idx)\n",
        "    print(\"Actual output:\", idx)\n",
        "\n",
        "    assert np.all(idx == expected_idx), f\"Test case 2 failed. Expected: {expected_idx}, got: {idx}\"\n",
        "\n",
        "    # Test case 3: More centroids than points\n",
        "    X = np.array([[1, 1], [4, 4]])\n",
        "    centroids = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n",
        "    idx = target(X, centroids)\n",
        "    expected_idx = np.array([0, 3])\n",
        "\n",
        "    print(\"\\nTest case 3:\")\n",
        "    print(\"Input X:\", X)\n",
        "    print(\"Input centroids:\", centroids)\n",
        "    print(\"Expected output:\", expected_idx)\n",
        "    print(\"Actual output:\", idx)\n",
        "\n",
        "    assert np.all(idx == expected_idx), f\"Test case 3 failed. Expected: {expected_idx}, got: {idx}\"\n",
        "\n",
        "    print(\"\\n\\033[92mAll tests passed!\")\n",
        "\n",
        "find_closest_centroids_test(find_closest_centroids)"
      ],
      "metadata": {
        "id": "P2rzRYKCeH9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPECTED OUTPUT**\n",
        "\n",
        "```\n",
        "Test case 1:\n",
        "Input X: [[1 1]\n",
        " [2 1]\n",
        " [4 3]\n",
        " [5 4]]\n",
        "Input centroids: [[1 2]\n",
        " [5 3]]\n",
        "Expected output: [0 0 1 1]\n",
        "\n",
        "Test case 2:\n",
        "Input X: [[1 1 1]\n",
        " [2 2 2]\n",
        " [4 4 4]\n",
        " [5 5 5]]\n",
        "Input centroids: [[1 1 1]\n",
        " [4 4 4]]\n",
        "Expected output: [0 0 1 1]\n",
        "\n",
        "Test case 3:\n",
        "Input X: [[1 1]\n",
        " [4 4]]\n",
        "Input centroids: [[1 1]\n",
        " [2 2]\n",
        " [3 3]\n",
        " [4 4]]\n",
        "Expected output: [0 3]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "LoBHyWq0gLxL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp2KuncRFlgt"
      },
      "source": [
        "### 1.2 Recalculating Centroid Averages\n",
        "\n",
        "Once every point has been assigned to a centroid, the next step of the algorithm involves recalculating the averages for each centroid based on the points allocated to it.\n",
        "\n",
        "<a name=\"ex02\"></a>\n",
        "### Exercise 2\n",
        "\n",
        "Please implement the function `compute_centroids` detailed below to update the position of each centroid.\n",
        "\n",
        "* Specifically, for each centroid $\\mu_k$, it is recalculated as follows:\n",
        "$$\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^{(i)}$$\n",
        "\n",
        "    where:\n",
        "    * $C_k$ represents the collection of data points assigned to centroid $k$.\n",
        "    * $|C_k|$ denotes the count of data points in $C_k$.\n",
        "\n",
        "* For example, if data points $x^{(3)}$ and $x^{(5)}$ are both assigned to centroid $k=2$, then centroid $\\mu_2$ should be updated to $\\mu_2 = \\frac{1}{2}(x^{(3)} + x^{(5)})$.\n",
        "\n",
        "This function recalculates each centroid based on the mean of the points assigned to it, ensuring that the centroids move towards the optimal position to minimize the distance to the points in their cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NCwIaUIeFlgu"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION 2: compute_centpods\n",
        "\n",
        "def compute_centroids(X, idx, K):\n",
        "    \"\"\"\n",
        "    Compute new centroids based on the current assignments\n",
        "\n",
        "    Input:\n",
        "    X: ndarray of shape (m, n) - m data points with n features\n",
        "    idx: ndarray of shape (m,) - current assignments\n",
        "    K: int - number of clusters\n",
        "\n",
        "    Output:\n",
        "    centroids: ndarray of shape (K, n) - updated centroids\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    centroids = np.zeros((K, n))\n",
        "\n",
        "### START CODE HERE ###\n",
        "    for k in range(K):\n",
        "        points_in_cluster =  # Your code here to get a list of all data points in X assigned to centroid k\n",
        "        if len(points_in_cluster) > 0: # a check to avoid division by zero when a cluster is empty\n",
        "            centroids[k] =  # Your code here to compute the mean of the points assigned\n",
        "  ### END CODE HERE ##\n",
        "    return centroids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST FUNCTION**"
      ],
      "metadata": {
        "id": "uTtEXoUVi1eG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGH4yWK4Flgu"
      },
      "outputs": [],
      "source": [
        "# Simply Run this cell.Do not Touch this code\n",
        "# Test function\n",
        "def compute_centroids_test(target):\n",
        "    print(\"Running tests for compute_centroids:\")\n",
        "\n",
        "    # Test case 1: Simple 2D example\n",
        "    X = np.array([[1, 1], [2, 1], [4, 3], [5, 4]])\n",
        "    idx = np.array([0, 0, 1, 1])\n",
        "    K = 2\n",
        "    centroids = target(X, idx, K)\n",
        "    expected_centroids = np.array([[1.5, 1], [4.5, 3.5]])\n",
        "\n",
        "    print(\"\\nTest case 1:\")\n",
        "    print(\"Input X:\", X)\n",
        "    print(\"Input idx:\", idx)\n",
        "    print(\"Input K:\", K)\n",
        "    print(\"Expected output:\", expected_centroids)\n",
        "    print(\"Actual output:\", centroids)\n",
        "\n",
        "    assert np.allclose(centroids, expected_centroids), f\"Test case 1 failed. Expected: {expected_centroids}, got: {centroids}\"\n",
        "\n",
        "    # Test case 2: 3D example\n",
        "    X = np.array([[1, 1, 1], [2, 2, 2], [4, 4, 4], [5, 5, 5]])\n",
        "    idx = np.array([0, 0, 1, 1])\n",
        "    K = 2\n",
        "    centroids = target(X, idx, K)\n",
        "    expected_centroids = np.array([[1.5, 1.5, 1.5], [4.5, 4.5, 4.5]])\n",
        "\n",
        "    print(\"\\nTest case 2:\")\n",
        "    print(\"Input X:\", X)\n",
        "    print(\"Input idx:\", idx)\n",
        "    print(\"Input K:\", K)\n",
        "    print(\"Expected output:\", expected_centroids)\n",
        "    print(\"Actual output:\", centroids)\n",
        "\n",
        "    assert np.allclose(centroids, expected_centroids), f\"Test case 2 failed. Expected: {expected_centroids}, got: {centroids}\"\n",
        "\n",
        "    # Test case 3: Empty cluster\n",
        "    X = np.array([[1, 1], [2, 2], [4, 4], [5, 5]])\n",
        "    idx = np.array([0, 0, 1, 1])\n",
        "    K = 3  # Note: K is 3, but we only have 2 clusters in idx\n",
        "    centroids = target(X, idx, K)\n",
        "    expected_centroids = np.array([[1.5, 1.5], [4.5, 4.5], [0, 0]])\n",
        "\n",
        "    print(\"\\nTest case 3:\")\n",
        "    print(\"Input X:\", X)\n",
        "    print(\"Input idx:\", idx)\n",
        "    print(\"Input K:\", K)\n",
        "    print(\"Expected output:\", expected_centroids)\n",
        "    print(\"Actual output:\", centroids)\n",
        "\n",
        "    assert np.allclose(centroids, expected_centroids), f\"Test case 3 failed. Expected: {expected_centroids}, got: {centroids}\"\n",
        "\n",
        "    print(\"\\n\\033[92mAll tests passed!\")\n",
        "compute_centroids_test(compute_centroids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPECTED OUTPUT**\n",
        "\n",
        "```\n",
        "Test case 1:\n",
        "Input X: [[1 1]\n",
        " [2 1]\n",
        " [4 3]\n",
        " [5 4]]\n",
        "Input idx: [0 0 1 1]\n",
        "Input K: 2\n",
        "Expected output: [[1.5 1. ]\n",
        " [4.5 3.5]]\n",
        "\n",
        "Test case 2:\n",
        "Input X: [[1 1 1]\n",
        " [2 2 2]\n",
        " [4 4 4]\n",
        " [5 5 5]]\n",
        "Input idx: [0 0 1 1]\n",
        "Input K: 2\n",
        "Expected output: [[1.5 1.5 1.5]\n",
        " [4.5 4.5 4.5]]\n",
        "\n",
        "Test case 3:\n",
        "Input X: [[1 1]\n",
        " [2 2]\n",
        " [4 4]\n",
        " [5 5]]\n",
        "Input idx: [0 0 1 1]\n",
        "Input K: 3\n",
        "Expected output: [[1.5 1.5]\n",
        " [4.5 4.5]\n",
        " [0.  0. ]]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "uDJArGZ8sAkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 The Elbow Curve Method\n",
        "\n",
        "Exercise 3:Implement Elbow Curve method\n",
        "\n",
        "### Description\n",
        "\n",
        "The Elbow Method is a heuristic used to determine the optimal number of clusters (K) in K-Means clustering. It helps to find the value of K that best fits the data without overfitting.\n",
        "\n",
        "By plotting the elbow curve, we can visually identify the point where increasing K further doesn't provide substantial improvements, thus helping us choose a suitable number of clusters for our data.\n",
        "\n",
        "In K-Means clustering, the goal is to partition the dataset into K clusters such that the sum of squared distances between data points and their corresponding cluster centroids is minimized.\n",
        "\n",
        "### Mathematical Expression for Distortion (Within-Cluster Sum of Squares)\n",
        "\n",
        "Distortion can also be expressed as the average of the squared distances:\n",
        "\n",
        "$$\\text{Distortion} = \\frac{1}{m} \\sum_{i=1}^m \\|X[i] - \\mu_{\\text{idx}[i]}\\|^2$$\n",
        "\n",
        "Where:\n",
        "- $m$: Number of data points\n",
        "- $X[i]$: Data point i\n",
        "- $\\mu_{\\text{idx}[i]}$: Centroid assigned to $X[i]$\n",
        "- $\\|\\cdot\\|^2$: Squared Euclidean norm\n",
        "\n",
        "### Implementing the Elbow Method\n",
        "\n",
        "1. Run K-Means for a range of K values (e.g., 1 to 10)\n",
        "2. For each K, compute the distortion\n",
        "3. Plot K versus distortion\n",
        "4. Identify the \"elbow\" point where the rate of decrease sharply changes\n",
        "5. This point suggests an optimal K, balancing between model complexity and data fitting"
      ],
      "metadata": {
        "id": "xO7UaB9lrZAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do not Touch\n",
        "%%capture\n",
        "!pip install kneed\n",
        "from kneed import KneeLocator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs"
      ],
      "metadata": {
        "id": "lHbKpTrIrYjn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_elbow(X, max_K):\n",
        "    \"\"\"\n",
        "    Plot the elbow curve to find the optimal K\n",
        "\n",
        "    Input:\n",
        "        X: ndarray of shape (m, n) - m data points with n features\n",
        "        max_K: int - maximum number of clusters to try\n",
        "\n",
        "    Output:\n",
        "        Elbow plot\n",
        "        optimal_k: int - optimal number of clusters\n",
        "    \"\"\"\n",
        "    distortions = []\n",
        "    K_range = range(1, max_K + 1)\n",
        "\n",
        "    for K in K_range:\n",
        "        centroids = X[np.random.choice(X.shape[0], K, replace=False)]\n",
        "\n",
        "        for _ in range(10):  # Run K-means for 10 iterations\n",
        "            idx = find_closest_centroids(X, centroids)\n",
        "            centroids = compute_centroids(X, idx, K)\n",
        "\n",
        "### START CODE HERE ###\n",
        "\n",
        "        distortion = # Your Code here to calculate the distortion for certian K\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "        distortions.append(distortion)\n",
        "\n",
        "    # Plot the distortion values against K\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(K_range, distortions, 'bx-')\n",
        "    plt.xlabel('Number of clusters (K)')\n",
        "    plt.ylabel('Average Distortion')\n",
        "    plt.title('Elbow Method for Optimal K')\n",
        "\n",
        "    # Use KneeLocator to find the elbow point\n",
        "    kneedle = KneeLocator(K_range, distortions, curve='convex', direction='decreasing')\n",
        "    optimal_k = kneedle.elbow\n",
        "\n",
        "    # Highlight the optimal K on the plot\n",
        "    plt.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal K = {optimal_k}')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"The optimal number of clusters (K) is: {optimal_k}\")\n",
        "    return optimal_k\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "# Find optimal K using the elbow method\n",
        "optimal_k = plot_elbow(X, max_K=10)\n",
        "print(f\"Optimal K: {optimal_k}\")"
      ],
      "metadata": {
        "id": "hQUhiZcatXDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST FUNCTION**"
      ],
      "metadata": {
        "id": "7CGxn-7rivQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simply Run this cell.Do not Touch this code\n",
        "# Test function\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "def plot_elbow_test(target):\n",
        "    print(\"Running tests for plot_elbow:\")\n",
        "\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "\n",
        "    # Test case 1: 3 clusters\n",
        "    X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)\n",
        "    max_K = 10\n",
        "\n",
        "    optimal_k = target(X, max_K)\n",
        "\n",
        "    print(\"\\nTest case 1 (3 clusters):\")\n",
        "    print(\"Input X shape:\", X.shape)\n",
        "    print(\"Input max_K:\", max_K)\n",
        "    print(\"Optimal K:\", optimal_k)\n",
        "\n",
        "    assert 2 <= optimal_k <= 4, f\"Test case 1 failed. Expected optimal K between 2 and 4, got: {optimal_k}\"\n",
        "\n",
        "    # Test case 2: 4 clusters\n",
        "    X, _ = make_blobs(n_samples=400, centers=4, cluster_std=0.60, random_state=0)\n",
        "    max_K = 10\n",
        "\n",
        "    optimal_k = target(X, max_K)\n",
        "\n",
        "    print(\"\\nTest case 2 (4 clusters):\")\n",
        "    print(\"Input X shape:\", X.shape)\n",
        "    print(\"Input max_K:\", max_K)\n",
        "    print(\"Optimal K:\", optimal_k)\n",
        "\n",
        "    assert 3 <= optimal_k <= 5, f\"Test case 2 failed. Expected optimal K between 3 and 5, got: {optimal_k}\"\n",
        "\n",
        "\n",
        "    print(\"\\n\\033[92mAll tests passed!\")\n",
        "\n",
        "# You can run the test like this:\n",
        "plot_elbow_test(plot_elbow)"
      ],
      "metadata": {
        "id": "4JlXRaeuvMwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeP-aPTzFlgu"
      },
      "source": [
        "## 2 - K-means on a Sample Dataset\n",
        "\n",
        "After completing the two functions (`find_closest_centroids` and `compute_centroids`), and plotting the elbow curve for finiding optimal clusters using 'plot_elbow'\n",
        "\n",
        "the next step is to run the K-means algorithm on a sample dataset and visualize the results. We'll use sklearn to generate our data and implement a method to find the optimal number of clusters.\n",
        "\n",
        "### Dataset Generation with sklearn\n",
        "\n",
        "We use sklearn's `make_blobs` function to create a synthetic dataset for our clustering task. This function generates isotropic Gaussian blobs for clustering, allowing us to create a controlled dataset with a known number of clusters.\n",
        "\n",
        "\n",
        "### Understanding the `run_kmeans` Function\n",
        "\n",
        "We encourage you to examine the `run_kmeans` function below to understand its workings:\n",
        "\n",
        "**Note**: You do not need to implement anything for this part. Simply run the code provided below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9XjptVBFlgu"
      },
      "outputs": [],
      "source": [
        "# You do not need to implement anything for this part\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "def run_kmeans(X, K, max_iter=100):\n",
        "    \"\"\"\n",
        "    Run the full K-means algorithm\n",
        "\n",
        "    Input:\n",
        "    X: ndarray of shape (m, n) - m data points with n features\n",
        "    K: int - number of clusters\n",
        "    max_iter: int - maximum number of iterations\n",
        "\n",
        "    Output:\n",
        "    centroids: ndarray of shape (K, n) - final centroids\n",
        "    idx: ndarray of shape (m,) - final assignments\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    centroids = X[np.random.choice(m, K, replace=False)]\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        idx = find_closest_centroids(X, centroids)\n",
        "        new_centroids = compute_centroids(X, idx, K)\n",
        "\n",
        "        if np.allclose(centroids, new_centroids):\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids\n",
        "\n",
        "    # Visualize results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=idx, cmap='viridis')\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
        "    plt.title(f'K-means Clustering (K={K})')\n",
        "    plt.show()\n",
        "\n",
        "    return centroids, idx\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate sample data\n",
        "    X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "    # Find optimal K using the elbow method\n",
        "    optimal_k = plot_elbow(X, max_K=10)\n",
        "    print(f\"Optimal K: {optimal_k}\")\n",
        "\n",
        "    # Run K-means with the optimal K\n",
        "    centroids, idx = run_kmeans(X, optimal_k)\n",
        "\n",
        "    print(\"K-means clustering completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wnn8Pmvw0Yep"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}